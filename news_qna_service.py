import os, re, threading
from typing import List, Dict, Any, Optional, TypedDict
import vertexai
from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput
from vertexai.generative_models import GenerativeModel
from qdrant_client import QdrantClient
from google.oauth2 import service_account
from collections.abc import Generator
from vertexai.generative_models import Candidate
import streamlit as st  # Streamlit secrets ì‚¬ìš© ì‹œ
from dotenv import load_dotenv

load_dotenv()

class RAGState(TypedDict):
    question: str
    documents: List[Dict[str, Any]]
    answer: Optional[str]

class NewsQnAService:
    """Qdrant + Gemini ê¸°ë°˜ RAG ì„œë¹„ìŠ¤"""
    _thread_local = threading.local()

    def __init__(
        self,
        project: str | None = None,
        location: str = "us-central1",
        qdrant_url: str | None = None,
        qdrant_key: str | None = None,
        collection: str = "stock_news",
        embed_model_name: str = "gemini-embedding-001",
        gen_model_name: str = "gemini-2.5-pro",
        embed_dim: int = 3072,
        top_k: int = 5,
        rerank_top_k: int = 5,
        use_rerank: bool = False,
    ):
        self.project = project or os.getenv("GOOGLE_CLOUD_PROJECT")
        self.location = location or os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
        if not self.project:
            raise RuntimeError("GOOGLE_CLOUD_PROJECT required")

        # ğŸ” Streamlit secretsì—ì„œ ì„œë¹„ìŠ¤ê³„ì • ì½ì–´ credentials ìƒì„±
        sa_info = None
        try:
            sa_info = st.secrets.get("gcp_service_account", None)
        except Exception:
            sa_info = None

        creds = None
        if sa_info:
            creds = service_account.Credentials.from_service_account_info(
                dict(sa_info),
                scopes=["https://www.googleapis.com/auth/cloud-platform"],
            )

        # âœ… credentialsê¹Œì§€ ëª…ì‹œí•´ì„œ ì´ˆê¸°í™”

        vertexai.init(project=self.project, location=self.location,credentials=creds)

        self.qdrant_url = qdrant_url or os.getenv("QDRANT_URL")
        self.qdrant_key = qdrant_key or os.getenv("QDRANT_API_KEY")
        if not (self.qdrant_url and self.qdrant_key):
            raise RuntimeError("QDRANT_URL / QDRANT_API_KEY required")

        self.collection = collection or os.getenv("COLLECTION_NAME", "stock_news")
        self.embed_model_name = embed_model_name or os.getenv("EMBED_MODEL_NAME", "gemini-embedding-001")
        self.gen_model_name = gen_model_name or os.getenv("GENAI_MODEL_NAME", "gemini-2.5-flash-lite")
        self.embed_dim = int(embed_dim or int(os.getenv("EMBED_DIM", "3072")))
        self.top_k = int(top_k or int(os.getenv("DEFAULT_TOP_K", "5")))
        self.rerank_top_k = int(rerank_top_k or int(os.getenv("RERANK_TOP_K", "5")))
        self.use_rerank = use_rerank

        self.qc = QdrantClient(url=self.qdrant_url, api_key=self.qdrant_key)

        # ëª¨ë¸ì€ ìŠ¤ë ˆë“œ ë¡œì»¬ ìºì‹±
        self._ensure_models()

    # ---------- internals ----------
    def _ensure_models(self):
        if not hasattr(self._thread_local, "embed_model") or getattr(self._thread_local, "embed_name", None) != self.embed_model_name:
            self._thread_local.embed_model = TextEmbeddingModel.from_pretrained(self.embed_model_name)
            self._thread_local.embed_name = self.embed_model_name
        if not hasattr(self._thread_local, "gen_model") or getattr(self._thread_local, "gen_name", None) != self.gen_model_name:
            self._thread_local.gen_model = GenerativeModel(self.gen_model_name)
            self._thread_local.gen_name = self.gen_model_name

    @property
    def embed_model(self) -> TextEmbeddingModel:
        self._ensure_models()
        return self._thread_local.embed_model

    @property
    def gen_model(self) -> GenerativeModel:
        self._ensure_models()
        return self._thread_local.gen_model

    def _embed_query(self, text: str) -> list[float]:
        inp = [TextEmbeddingInput(text=text or "", task_type="RETRIEVAL_QUERY")]
        return self.embed_model.get_embeddings(inp, output_dimensionality=self.embed_dim)[0].values

    # ---------- RAG steps ----------
    
    # def _extract_text_from_payload(self, payload: dict) -> str:
    #     """
    #     payload["doc"]ê°€ ë¬¸ìì—´ì´ê±°ë‚˜, dict(ì˜ˆ: {"content": "...", "text": "...", ...})ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ëª¨ë‘ ì»¤ë²„
    #     """
    #     if not isinstance(payload, dict):
    #         return ""
    #     doc = payload.get("doc")
    #     if isinstance(doc, str):
    #         return doc
    #     if isinstance(doc, dict):
    #         # í”í•œ í…ìŠ¤íŠ¸ í‚¤ë“¤ ìš°ì„ ìˆœìœ„
    #         return doc.get("content") or doc.get("text") or doc.get("page_content") or ""
    #     return ""

    def _extract_text_from_payload(self, payload: dict) -> str:
        """
        (text, title, link) ì¶”ì¶œ:
        1) payload["doc"] (str/dict)
        2) payload["metadata"] (dict)
        3) payload ìƒìœ„ í‚¤
        ìš°ì„ ìˆœìœ„ë¡œ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¨ë‹¤.
        """
        if not isinstance(payload, dict):
            return "","",""
        text = ""
        title = ""
        link = ""
        
        doc = payload.get("doc")

        if isinstance(doc, str):
            return doc
        elif isinstance(doc, dict):
            text  = doc.get("text") or doc.get("content") or doc.get("page_content") or ""
            title = doc.get("title") or title
            link  = doc.get("link")  or doc.get("url") or link

        # 2) metadata
        md = payload.get("metadata") if isinstance(payload.get("metadata"), dict) else {}
        if not text:
            text = md.get("text") or md.get("content") or md.get("page_content") or ""
        if not title:
            title = md.get("title") or md.get("headline") or md.get("subject") or ""
        if not link:
            link = md.get("link") or md.get("url") or ""

        # 3) payload ìƒìœ„ ë³´ê°•
        if not title:
            title = payload.get("title") or title
        if not link:
            link = payload.get("link") or payload.get("url") or link
        if not text:
            text = payload.get("text") or payload.get("content") or payload.get("page_content") or text

        return text, title, link
    # title, link ì¶”ê°€ ì „
    # def retrieve(self, question: str) -> List[Dict[str, Any]]:
    #     qv = self._embed_query(question)
    #     hits = self.qc.search(
    #         collection_name=self.collection,
    #         query_vector=qv,
    #         limit=self.top_k if not self.use_rerank else self.rerank_top_k,
    #         with_payload=True,
    #         with_vectors=False,
    #     )
    
    #     # (ì„ íƒ) distance ëª¨ë“œ íŒŒì•…
    #     dist_mode = getattr(self, "_dist_mode", None)
    #     if dist_mode is None:
    #         try:
    #             info = self.qc.get_collection(self.collection)
    #             params = getattr(info.config, "params", None) or getattr(info, "config", None)
    #             vectors = getattr(params, "vectors", None)
    #             dist_mode = str(getattr(vectors, "distance", "")).lower() if vectors else ""
    #         except Exception:
    #             dist_mode = ""
    #         self._dist_mode = dist_mode
    
    #     docs: List[Dict[str, Any]] = []
    #     for h in hits:
    #         payload = h.payload or {}
    #         text = self._extract_text_from_payload(payload)
    
    #         # ë©”íƒ€ë°ì´í„°: payload["metadata"] ìµœìš°ì„ , ì—†ìœ¼ë©´ payloadì—ì„œ doc ì œì™¸
    #         md = {}
    #         if isinstance(payload.get("metadata"), dict):
    #             md = dict(payload["metadata"])
    #         else:
    #             md = {k: v for k, v in payload.items() if k not in ("doc", "metadata")}
    
    #         raw = getattr(h, "score", None)  # QdrantëŠ” ë³´í†µ distanceë¥¼ scoreë¡œ ë°˜í™˜
    #         distance = float(raw) if raw is not None else None
    #         similarity = None
    #         if distance is not None and "cosine" in dist_mode:
    #             similarity = distance
    
    #         docs.append({
    #             "id": str(getattr(h, "id", "")),
    #             "content": text,            # âœ… ì´ì œ doc ê¸°ë°˜ ë³¸ë¬¸
    #             #"metadata": md,             # âœ… metadata ê·¸ëŒ€ë¡œ
    #             "title": title,
    #             "link": link,
    #             "score": similarity if similarity is not None else (float(raw) if raw is not None else None),
    #             "distance": distance,
    #             "distance_mode": dist_mode,
    #         })
    #     return docs
    def retrieve(self, question: str) -> List[Dict[str, Any]]:
        qv = self._embed_query(question)
        hits = self.qc.search(
            collection_name=self.collection,
            query_vector=qv,
            limit=self.top_k if not self.use_rerank else self.rerank_top_k,
            with_payload=True,
            with_vectors=False,
        )

        # ê±°ë¦¬/ìŠ¤ì½”ì–´ ëª¨ë“œ íŒŒì•…
        dist_mode = getattr(self, "_dist_mode", None)
        if dist_mode is None:
            try:
                info = self.qc.get_collection(self.collection)
                params = getattr(info.config, "params", None) or getattr(info, "config", None)
                vectors = getattr(params, "vectors", None)
                dist_mode = str(getattr(vectors, "distance", "")).lower() if vectors else ""
            except Exception:
                dist_mode = ""
            self._dist_mode = dist_mode

        docs: List[Dict[str, Any]] = []
        for h in hits:
            payload = h.payload or {}

            # âœ… ì—¬ê¸°ì„œ í†µí•© ì¶”ì¶œ
            text, title, link = self._extract_text_from_payload(payload)

            # metadata êµ¬ì„±
            if isinstance(payload.get("metadata"), dict):
                md = dict(payload["metadata"])
            else:
                md = {k: v for k, v in payload.items() if k not in ("doc", "metadata")}

            raw_score = getattr(h, "score", None)
            score = float(raw_score) if raw_score is not None else None

            # âœ… Qdrantì˜ scoreëŠ” "í´ìˆ˜ë¡ ë” ìœ ì‚¬"ê°€ ë˜ë„ë¡ ì •ì˜ë¨.
            #    cosine/dotì€ scoreë¥¼ ê·¸ëŒ€ë¡œ similarityë¡œ ì“°ëŠ” ê²Œ ì¼ë°˜ì .
            #    euclidì¼ ë• ê´€ë¡€ìƒ -distanceê°€ scoreì¸ ê²½ìš°ê°€ ë§ì•„ ë³„ë„ ê°€ê³µ ì—†ì´ í‘œê¸°ë§Œ í•˜ê±°ë‚˜ None ì²˜ë¦¬.
            similarity = None
            if score is not None:
                if "cosine" in dist_mode or "dot" in dist_mode:
                    similarity = score
                else:
                    similarity = None  # í•„ìš”í•˜ë©´ -score ë“±ìœ¼ë¡œ í™˜ì‚° ì •ì±… ê²°ì •

            docs.append({
                "id": str(getattr(h, "id", "")),
                "content": text,        # âœ… ì´ì œ ë¹ˆ ê°’ì´ ì•„ë‹˜ (metadataê¹Œì§€ ì»¤ë²„)
                "title": title,
                "link": link,
                "metadata": md,
                "score": similarity if similarity is not None else score,
                "distance": score,      # í˜¼ë™ ë°©ì§€ë¥¼ ì›í•˜ë©´ ì´ í•„ë“œëª…ì€ ë¹¼ê±°ë‚˜ 'raw_score'ë¡œ ë³€ê²½ ê¶Œì¥
                "distance_mode": dist_mode,
            })

        return docs

    def rerank(self, question: str, docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        # ìë¦¬ë§Œë“¤ê¸°: í•„ìš”ì‹œ Vertex Rankingì´ë‚˜ cross-encoder ë¶™ì´ê¸°
        # ì§€ê¸ˆì€ ê·¸ëŒ€ë¡œ top_k ìƒìœ„ë§Œ ë¦¬í„´
        return (docs or [])[: self.top_k]

 # 5. ëª¨í˜¸í•˜ê±°ë‚˜ ê·¼ê±° ì—†ëŠ” ë‚´ìš©ì€ ì“°ì§€ ë§ê³  "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
    def generate(self, question: str, docs: List[Dict[str, Any]]) -> str:
        if not docs:
            return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        # ctx = "\n\n".join(d["content"] for d in docs)
        ctx = "\n\n".join(f"""ì œëª©: {d["title"]}
ë³¸ë¬¸: {d["content"]}
url: {d["link"]}""" for d in docs)
        
        prompt = f"""
      ë‹¹ì‹ ì€ ì£¼ì‹ì‹œì¥ê³¼ ì—°ê¸ˆì— ì •í†µí•œ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 
      ë‹¹ì‹ ì—ê²Œ ì£¼ì‹ ì¢…ëª©ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ê¸°ì‚¬ê°€ ì œê³µë©ë‹ˆë‹¤. 
      ì•„ë˜ ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ê·¼ê±°ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë§ëŠ” í•œêµ­ì–´ ë‹µë³€ì„ ì¶©ì‹¤í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
      ë‹µë³€ì„ ì‘ì„± ì‹œ ì•„ë˜ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”.
      
        [ì‘ì„± ì§€ì¹¨]  
        1. ë‹µë³€ì€ **3ë‹¨ë½ ì´ìƒ**ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.  
        2. **ì¤‘ìš” í¬ì¸íŠ¸ëŠ” êµµê²Œ**, í•µì‹¬ ìˆ˜ì¹˜ëŠ” `ì½”ë“œë¸”ë¡ ìŠ¤íƒ€ì¼`ë¡œ í‘œì‹œí•˜ì„¸ìš”.  
        3. ë‹µë³€ ì¤‘ê°„ì—ëŠ” â–¸, âœ”, âœ¦ ê°™ì€ ë¶ˆë¦¿ ì•„ì´ì½˜ì„ í™œìš©í•´ ì‹œê°ì ìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•˜ì„¸ìš”.  
        4. ë§ˆì§€ë§‰ì— `---` êµ¬ë¶„ì„ ì„ ë„£ê³ , ì œëª©ê³¼ urlì„ ì²¨ë¶€í•´ì£¼ì„¸ìš”
        5. ë‹µë³€ì— ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•´ì£¼ì„¸ìš”.
        
        [ë‰´ìŠ¤ê¸°ì‚¬]
        {ctx}
        
        [ì§ˆë¬¸]
        {question}
        """
        try:
            resp = self.gen_model.generate_content(prompt, generation_config={"temperature": 0.2})
            return (resp.text or "").strip()
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def generate_stream(self, question: str, docs: List[Dict[str, Any]]) -> Generator[str, None, None]:
        """Gemini ìŠ¤íŠ¸ë¦¬ë° ì œë„ˆë ˆì´í„°: chunk ë¬¸ìì—´ì„ yield"""
        if not docs:
            yield "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return

        #ctx = "\n\n".join(d["content"] for d in docs)
        ctx = "\n\n".join(f"""ì œëª©: {d["title"]}
ë³¸ë¬¸: {d["content"]}
url: {d["link"]}""" for d in docs)
        prompt = f"""
      ë‹¹ì‹ ì€ ì£¼ì‹ì‹œì¥ê³¼ ì—°ê¸ˆì— ì •í†µí•œ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
      ë‹¹ì‹ ì—ê²Œ ì£¼ì‹ ì¢…ëª©ê³¼ ê´€ë ¨ëœ ë‰´ìŠ¤ê¸°ì‚¬ê°€ ì œê³µë©ë‹ˆë‹¤. 
      ì•„ë˜ ë‰´ìŠ¤ê¸°ì‚¬ë¥¼ ê·¼ê±°ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë§ëŠ” í•œêµ­ì–´ ë‹µë³€ì„ ì¶©ì‹¤í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
      
        [ì‘ì„± ì§€ì¹¨]  
        1. ë‹µë³€ì€ **3ë‹¨ë½ ì´ìƒ**ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.  
        2. **ì¤‘ìš” í¬ì¸íŠ¸ëŠ” êµµê²Œ**, í•µì‹¬ ìˆ˜ì¹˜ëŠ” `ì½”ë“œë¸”ë¡ ìŠ¤íƒ€ì¼`ë¡œ í‘œì‹œí•˜ì„¸ìš”.  
        3. ë‹µë³€ ì¤‘ê°„ì—ëŠ” â–¸, âœ”, âœ¦ ê°™ì€ ë¶ˆë¦¿ ì•„ì´ì½˜ì„ í™œìš©í•´ ì‹œê°ì ìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•˜ì„¸ìš”.  
        4. ë§ˆì§€ë§‰ì— `---` êµ¬ë¶„ì„ ì„ ë„£ê³ , ì œëª©ê³¼ urlì„ ì²¨ë¶€í•´ì£¼ì„¸ìš”.
        5. ë§Œì•½ ì œëª©ê³¼ urlì´ ë¹„ì–´ ìˆë‹¤ë©´ ë³„ë„ë¡œ ë‹µë³€ì— í¬í•¨ì‹œì¼œì„œëŠ” ì•ˆë©ë‹ˆë‹¤.
        5. ë‹µë³€ì— ì ì ˆí•œ ì´ëª¨ì§€ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì ìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•˜ì„¸ìš”.
        6. ë‹µë³€ì„ í•  ë•Œ ë‚´ìš©ì— ë§ëŠ” ì ì ˆí•œ ì†Œì œëª©ì„ ë¶™ì—¬ ì£¼ì„¸ìš”.
        
        ì•„ë˜ëŠ” ë‹¹ì‹ ì´ ìˆ˜í–‰í•´ì•¼í•  ì—…ë¬´ì˜ ìƒì„± ì˜ˆì‹œì…ë‹ˆë‹¤.

### ì‹¤ì  ë¶€ì§„ì—ë„ ê¸ì •ì ì¸ ì£¼ê°€ ì „ë§ ğŸ“ˆ

ìµœê·¼ ì‚¼ì„±ì „ìëŠ” ì‹œì¥ ì „ë§ì¹˜ë¥¼ ë°‘ë„ëŠ” `4ì¡° 6000ì–µì›`ì˜ 2ë¶„ê¸° ì ì • ì‹¤ì ì„ ë°œí‘œí•˜ë©° ë¶€ì§„í•œ ëª¨ìŠµì„ ë³´ì˜€ìŠµë‹ˆë‹¤. 
í•˜ì§€ë§Œ ë†€ëê²Œë„ ì£¼ê°€ëŠ” í° íƒ€ê²©ì„ ë°›ì§€ ì•Šì•˜ìœ¼ë©°, ì˜¤íˆë ¤ ì¦ê¶Œê°€ì—ì„œëŠ” ê¸ì •ì ì¸ ì „ë§ì„ ìŸì•„ë‚´ê³  ìˆìŠµë‹ˆë‹¤. 
ë‹¤ìˆ˜ì˜ ì• ë„ë¦¬ìŠ¤íŠ¸ë“¤ì€ **ì‚¼ì„±ì „ìê°€ 'ì‹¤ì  ë°”ë‹¥'ì„ í†µê³¼í•˜ê³  ìˆìœ¼ë©°, ì´ì œ ë³¸ê²©ì ì¸ íšŒë³µ êµ­ë©´ì— ì§„ì…í–ˆë‹¤**ê³  í‰ê°€í•©ë‹ˆë‹¤. 
ì´ëŸ¬í•œ ê¸°ëŒ€ê°ì„ ë°”íƒ•ìœ¼ë¡œ KBì¦ê¶Œ, ì‹ í•œíˆ¬ìì¦ê¶Œ ë“±ì€ ëª©í‘œ ì£¼ê°€ë¥¼ `9ë§Œì›`ìœ¼ë¡œ ìƒí–¥ ì¡°ì •í–ˆìœ¼ë©°, í‚¤ì›€ì¦ê¶Œ ì—­ì‹œ ëª©í‘œê°€ë¥¼ `8ë§Œ 9000ì›`ìœ¼ë¡œ ë†’ì—¬ ì¡ëŠ” ë“± '8ë§Œì „ì'ë¥¼ ë„˜ì–´ì„  ìƒìŠ¹ ê°€ëŠ¥ì„±ì— ë¬´ê²Œë¥¼ ì‹£ê³  ìˆìŠµë‹ˆë‹¤.

### ë¯¸ë˜ ì„±ì¥ì„ ì´ëŒ í•µì‹¬ ë™ë ¥: HBMê³¼ íŒŒìš´ë“œë¦¬ ğŸ¤–

ì• ë„ë¦¬ìŠ¤íŠ¸ë“¤ì´ ì‚¼ì„±ì „ìì˜ ë¯¸ë˜ë¥¼ ë°ê²Œ ë³´ëŠ” ê°€ì¥ í° ì´ìœ ëŠ” ë°”ë¡œ **ë°˜ë„ì²´ ì‚¬ì—…ë¶€ì˜ ê²½ìŸë ¥ íšŒë³µì— ëŒ€í•œ ê°•í•œ ë¯¿ìŒ** ë•Œë¬¸ì…ë‹ˆë‹¤. 
íŠ¹íˆ ì¸ê³µì§€ëŠ¥(AI) ì‹œëŒ€ì˜ í•µì‹¬ ë¶€í’ˆìœ¼ë¡œ ë– ì˜¤ë¥¸ ê³ ëŒ€ì—­í­ ë©”ëª¨ë¦¬(HBM)ì™€ íŒŒìš´ë“œë¦¬(ë°˜ë„ì²´ ìœ„íƒìƒì‚°) ë¶€ë¬¸ì´ ì„±ì¥ì„ ê²¬ì¸í•  ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.

â–¸ **HBM ê²½ìŸë ¥ íšŒë³µ**: í˜„ì¬ ìœ í†µë˜ëŠ” HBM3Eì˜ ë‹¤ìŒ ì„¸ëŒ€ì¸ **HBM4ì˜ ìƒ˜í”Œ ê³µê¸‰ì„ ì•ë‘ê³  ìˆìœ¼ë©°, í’ˆì§ˆ ë˜í•œ ê¸°ëŒ€ ì´ìƒì˜ ëª¨ìŠµ**ì„ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ íŒŒì•…ë©ë‹ˆë‹¤. ì´ëŠ” AI ë°˜ë„ì²´ ì‹œì¥ì—ì„œ ë‹¤ì‹œ ë¦¬ë”ì‹­ì„ íšŒë³µí•  ìˆ˜ ìˆë‹¤ëŠ” ì¤‘ìš”í•œ ì‹ í˜¸ì…ë‹ˆë‹¤.
âœ” **íŒŒìš´ë“œë¦¬ ì‚¬ì—… ë°˜ë“±**: ìµœê·¼ **í…ŒìŠ¬ë¼ì™€ì˜ íŒŒìš´ë“œë¦¬ ê³„ì•½ì„ í™•ë³´í•œ ê²ƒì€ ì‚¼ì„±ì „ìì˜ ì„œë¸Œ 5ë‚˜ë…¸ ê³µì • ê¸°ìˆ ë ¥ì„ ì…ì¦**í•˜ëŠ” ì¤‘ìš”í•œ ì„±ê³¼ì…ë‹ˆë‹¤. í–¥í›„ 2ë‚˜ë…¸ ê³µì • ê°œì„ ì„ í†µí•´ ì¶”ê°€ì ì¸ ëŒ€í˜• ê³ ê°ì‚¬ë¥¼ í™•ë³´í•  ê°€ëŠ¥ì„±ì´ ì»¤ì§€ë©´ì„œ íŒŒìš´ë“œë¦¬ ë¶€ë¬¸ì˜ ì ì ì¶•ì†Œì™€ ì‹¤ì  ê°œì„ ì´ ê¸°ëŒ€ë©ë‹ˆë‹¤.
âœ¦ **ê²¬ì¡°í•œ ì™¸êµ­ì¸ ë§¤ìˆ˜ì„¸ì™€ ì£¼ì£¼í™˜ì›**: ì´ì¬ìš© íšŒì¥ì˜ ì‚¬ë²• ë¦¬ìŠ¤í¬ í•´ì†Œì™€ í•¨ê»˜ ì™¸êµ­ì¸ íˆ¬ììë“¤ì˜ ë§¤ìˆ˜ì„¸ê°€ ê¾¸ì¤€íˆ ìœ ì…ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì— **ìì‚¬ì£¼ ì¶”ê°€ ë§¤ì… ë° ì†Œê° ë“± ì£¼ì£¼í™˜ì› ì •ì±…ì— ëŒ€í•œ ê¸°ëŒ€ê°** ì—­ì‹œ ì£¼ê°€ì— ê¸ì •ì ì¸ ìš”ì¸ìœ¼ë¡œ ì‘ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### ê³¼ê±°ë¶€í„° ì´ì–´ì§„ 'ì €í‰ê°€' ë§¤ë ¥ê³¼ ë¦¬ìŠ¤í¬ ìš”ì¸ ğŸ§

ì‚¼ì„±ì „ìì˜ 'ì €í‰ê°€' ë§¤ë ¥ì€ ì–´ì œì˜¤ëŠ˜ì˜ ì´ì•¼ê¸°ê°€ ì•„ë‹™ë‹ˆë‹¤. 
ê¸°ì‚¬ì— ë”°ë¥´ë©´, ì£¼ê°€ê°€ ì‚¬ìƒ ìµœê³ ì¹˜ë¥¼ ê²½ì‹ í•˜ë˜ 2016ë…„ì—ë„ ìì‚°ìš´ìš©ì‚¬ë“¤ì€ **"ì—°ê°„ ì˜ì—…ì´ìµì´ `40ì¡°ì›`ì— ë‹¬í•˜ì§€ë§Œ ì‹œê°€ì´ì•¡ì€ `260ì¡°ì›`ì— ë¶ˆê³¼í•˜ë‹¤"**ë©° í•´ì™¸ ê²½ìŸì‚¬ ëŒ€ë¹„ í•­ìƒ ì €í‰ê°€ë˜ì–´ ì™”ë‹¤ê³  ë¶„ì„í–ˆìŠµë‹ˆë‹¤. 
ë‹¹ì‹œ ì£¼ê°€ìˆ˜ìµë¹„ìœ¨(PER)ì´ 10ë°° ë¯¸ë§Œì´ì—ˆë˜ ì ì„ ê³ ë ¤í•˜ë©´, í˜„ì¬ì˜ ì‹¤ì  íšŒë³µ ê¸°ëŒ€ê°ì€ ì£¼ê°€ ìƒìŠ¹ ì—¬ë ¥ì´ ì¶©ë¶„í•˜ë‹¤ëŠ” ê·¼ê±°ê°€ ë©ë‹ˆë‹¤.

ë¬¼ë¡  ë¦¬ìŠ¤í¬ ìš”ì¸ë„ ì¡´ì¬í•©ë‹ˆë‹¤. 
ìŠ¤ë§ˆíŠ¸í°ì˜ ë‘ë‡Œ ì—­í• ì„ í•˜ëŠ” AP(ì• í”Œë¦¬ì¼€ì´ì…˜ í”„ë¡œì„¸ì„œ)ë¥¼ í€„ì»´ì— ì „ì ìœ¼ë¡œ ì˜ì¡´í•˜ê²Œ ë˜ë©´ì„œ **ê°€ê²© í˜‘ìƒë ¥ì´ ë–¨ì–´ì ¸ ìˆ˜ìµì„± í™•ë³´ì— ì–´ë ¤ì›€ì„ ê²ªì„ ìˆ˜ ìˆë‹¤**ëŠ” ì ì€ ìš°ë ¤ìŠ¤ëŸ¬ìš´ ë¶€ë¶„ì…ë‹ˆë‹¤. 
ë˜í•œ, ì• ë„ë¦¬ìŠ¤íŠ¸ë“¤ì€ ì‚¼ì„±ì „ìì˜ ì£¼ê°€ê°€ ì‹¤ì ê³¼ ê±°ì˜ ì™„ë²½í•˜ê²Œ ë™í–‰í•˜ê¸° ë•Œë¬¸ì—, ë§Œì•½ ê¸°ëŒ€í–ˆë˜ ë°˜ë„ì²´ ë¶€ë¬¸ì˜ ì‹¤ì  ê°œì„ ì´ ì§€ì—°ë  ê²½ìš° ì–¸ì œë“  ì£¼ê°€ê°€ í•˜ë½í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì„ í•­ìƒ ìœ ë…í•´ì•¼ í•œë‹¤ê³  ì¡°ì–¸í•©ë‹ˆë‹¤. âš ï¸

---
**[ê·¼ê±° ê¸°ì‚¬]**
- ì‚¼ì„±ì „ì â€˜ê¸°íšŒì˜ ìˆœê°„ 5â€™ [ìŠ¤í˜ì…œë¦¬í¬íŠ¸] (url: https://n.news.naver.com/mnews/article/024/0000098907?sid=101)
- Chip giants accelerate efforts to develop HBM alternatives (url: https://n.news.naver.com/mnews/article/009/0005506084?sid=104)
- [PRNewswire] í’€ë¬´ì›, ì‹ ì† ë°˜ì‘í˜• ê³µê¸‰ë§ êµ¬ì¶•ì„ ìœ„í•´ í‚¤ë„¥ì‹œìŠ¤ì™€ íŒŒíŠ¸... (url: https://n.news.naver.com/mnews/article/001/0010073574?sid=104)

        
[ë‰´ìŠ¤ê¸°ì‚¬]
{ctx}

[ì§ˆë¬¸]
{question}
"""
        try:
            responses = self.gen_model.generate_content(
                prompt,
                stream=True,
                generation_config={"temperature": 0.2},
            )

            # ì¼ë¶€ SDK ë²„ì „ì—ì„œ ì‘ë‹µì´ í›„ë³´/íŒŒì¸ ë¡œ ë‚˜ë‰˜ê±°ë‚˜ response.textì— ë¸íƒ€ê°€ ë“¤ì–´ì˜µë‹ˆë‹¤.
            for res in responses:
                # 1) ê°€ì¥ ê°„ë‹¨: ë¸íƒ€ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ
                if getattr(res, "text", None):
                    yield res.text
                    continue

                # 2) í›„ë³´/íŒŒì¸ ì—ì„œ í…ìŠ¤íŠ¸ ì¶•ì¶œ
                chunk = []
                for cand in (getattr(res, "candidates", None) or []):
                    if isinstance(cand, Candidate) and getattr(cand, "content", None):
                        for part in getattr(cand.content, "parts", []) or []:
                            t = getattr(part, "text", None)
                            if t:
                                chunk.append(t)
                if chunk:
                    yield "".join(chunk)

        except Exception as e:
            yield f"\n\n(ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e})"

    # ---------- public APIs ----------
    def answer(self, question: str) -> Dict[str, Any]:
        docs = self.retrieve(question)
        docs = self.rerank(question, docs) if self.use_rerank else docs[: self.top_k]
        ans = self.generate(question, docs)
        return {"answer": ans, "source_documents": docs}

    def answer_stream(self, question: str):
        """retrieve â†’ (ì˜µì…˜) rerank â†’ stream ìƒì„±ê¸°ë¥¼ ë°˜í™˜"""
        docs = self.retrieve(question)
        docs = self.rerank(question, docs) if self.use_rerank else docs[: self.top_k]
        return self.generate_stream(question, docs)
    
    def retrieve_only(self, question: str, top_k: int | None = None) -> List[Dict[str, Any]]:
        prev_top_k, self.top_k = self.top_k, (top_k or self.top_k)
        try:
            docs = self.retrieve(question)
            return docs[: (top_k or self.top_k)]
        finally:
            self.top_k = prev_top_k

"""
if __name__  == "__main__":
    newsqa = NewsQnAService()
    doc_res = newsqa.retrieve_only("ì‚¼ì„±ì „ì ì£¼ê°€ ì „ë§ì€?")
    
    print("ë¬¸ì„œê²€ìƒ‰ ê²°ê³¼")
    print(doc_res)
    print("---")
    
    print("ì •ë‹µ ê²°ê³¼")
    result_stream = newsqa.answer_stream("ì‚¼ì„±ì „ì ì£¼ê°€ ì „ë§ì€?")
    
    # ì œë„ˆë ˆì´í„° ê°ì²´ë¥¼ ë°˜ë³µí•˜ì—¬ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ìˆœì„œëŒ€ë¡œ ì¶œë ¥
    for chunk in result_stream:
        print(chunk, end="") # end=""ë¥¼ ì‚¬ìš©í•´ ì¤„ë°”ê¿ˆ ì—†ì´ ì´ì–´ë¶™ì„
    print() # ë§ˆì§€ë§‰ì— ì¤„ë°”ê¿ˆ ì¶”ê°€
"""
