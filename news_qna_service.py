import os, re, threading
from typing import List, Dict, Any, Optional, TypedDict
import vertexai
from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput
from vertexai.generative_models import GenerativeModel
from qdrant_client import QdrantClient
from google.oauth2 import service_account
from collections.abc import Generator
from vertexai.generative_models import Candidate
import streamlit as st  # Streamlit secrets ì‚¬ìš© ì‹œ


class RAGState(TypedDict):
    question: str
    documents: List[Dict[str, Any]]
    answer: Optional[str]

class NewsQnAService:
    """Qdrant + Gemini ê¸°ë°˜ RAG ì„œë¹„ìŠ¤"""
    _thread_local = threading.local()

    def __init__(
        self,
        project: str | None = None,
        location: str = "us-central1",
        qdrant_url: str | None = None,
        qdrant_key: str | None = None,
        collection: str = "stock_news",
        embed_model_name: str = "gemini-embedding-001",
        gen_model_name: str = "gemini-2.5-flash-lite",
        embed_dim: int = 3072,
        top_k: int = 5,
        rerank_top_k: int = 5,
        use_rerank: bool = False,
    ):
        self.project = project or os.getenv("GOOGLE_CLOUD_PROJECT")
        self.location = location or os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
        if not self.project:
            raise RuntimeError("GOOGLE_CLOUD_PROJECT required")

        # ğŸ” Streamlit secretsì—ì„œ ì„œë¹„ìŠ¤ê³„ì • ì½ì–´ credentials ìƒì„±
        sa_info = None
        try:
            sa_info = st.secrets.get("gcp_service_account", None)
        except Exception:
            sa_info = None

        creds = None
        if sa_info:
            creds = service_account.Credentials.from_service_account_info(
                dict(sa_info),
                scopes=["https://www.googleapis.com/auth/cloud-platform"],
            )

        # âœ… credentialsê¹Œì§€ ëª…ì‹œí•´ì„œ ì´ˆê¸°í™”

        vertexai.init(project=self.project, location=self.location,credentials=creds)

        self.qdrant_url = qdrant_url or os.getenv("QDRANT_URL")
        self.qdrant_key = qdrant_key or os.getenv("QDRANT_API_KEY")
        if not (self.qdrant_url and self.qdrant_key):
            raise RuntimeError("QDRANT_URL / QDRANT_API_KEY required")

        self.collection = collection or os.getenv("COLLECTION_NAME", "stock_news")
        self.embed_model_name = embed_model_name or os.getenv("EMBED_MODEL_NAME", "gemini-embedding-001")
        self.gen_model_name = gen_model_name or os.getenv("GENAI_MODEL_NAME", "gemini-2.5-flash-lite")
        self.embed_dim = int(embed_dim or int(os.getenv("EMBED_DIM", "3072")))
        self.top_k = int(top_k or int(os.getenv("DEFAULT_TOP_K", "5")))
        self.rerank_top_k = int(rerank_top_k or int(os.getenv("RERANK_TOP_K", "5")))
        self.use_rerank = use_rerank

        self.qc = QdrantClient(url=self.qdrant_url, api_key=self.qdrant_key)

        # ëª¨ë¸ì€ ìŠ¤ë ˆë“œ ë¡œì»¬ ìºì‹±
        self._ensure_models()

    # ---------- internals ----------
    def _ensure_models(self):
        if not hasattr(self._thread_local, "embed_model") or getattr(self._thread_local, "embed_name", None) != self.embed_model_name:
            self._thread_local.embed_model = TextEmbeddingModel.from_pretrained(self.embed_model_name)
            self._thread_local.embed_name = self.embed_model_name
        if not hasattr(self._thread_local, "gen_model") or getattr(self._thread_local, "gen_name", None) != self.gen_model_name:
            self._thread_local.gen_model = GenerativeModel(self.gen_model_name)
            self._thread_local.gen_name = self.gen_model_name

    @property
    def embed_model(self) -> TextEmbeddingModel:
        self._ensure_models()
        return self._thread_local.embed_model

    @property
    def gen_model(self) -> GenerativeModel:
        self._ensure_models()
        return self._thread_local.gen_model

    def _embed_query(self, text: str) -> list[float]:
        inp = [TextEmbeddingInput(text=text or "", task_type="RETRIEVAL_QUERY")]
        return self.embed_model.get_embeddings(inp, output_dimensionality=self.embed_dim)[0].values

    # ---------- RAG steps ----------
    def _extract_text_from_payload(self, payload: dict) -> str:
        """
        payload["doc"]ê°€ ë¬¸ìì—´ì´ê±°ë‚˜, dict(ì˜ˆ: {"content": "...", "text": "...", ...})ì¼ ìˆ˜ ìˆìœ¼ë‹ˆ ëª¨ë‘ ì»¤ë²„
        """
        if not isinstance(payload, dict):
            return ""
        doc = payload.get("doc")
        if isinstance(doc, str):
            return doc
        if isinstance(doc, dict):
            # í”í•œ í…ìŠ¤íŠ¸ í‚¤ë“¤ ìš°ì„ ìˆœìœ„
            return doc.get("content") or doc.get("text") or doc.get("page_content") or ""
        return ""
    
    def retrieve(self, question: str) -> List[Dict[str, Any]]:
        qv = self._embed_query(question)
        hits = self.qc.search(
            collection_name=self.collection,
            query_vector=qv,
            limit=self.top_k if not self.use_rerank else self.rerank_top_k,
            with_payload=True,
            with_vectors=False,
        )
    
        # (ì„ íƒ) distance ëª¨ë“œ íŒŒì•…
        dist_mode = getattr(self, "_dist_mode", None)
        if dist_mode is None:
            try:
                info = self.qc.get_collection(self.collection)
                params = getattr(info.config, "params", None) or getattr(info, "config", None)
                vectors = getattr(params, "vectors", None)
                dist_mode = str(getattr(vectors, "distance", "")).lower() if vectors else ""
            except Exception:
                dist_mode = ""
            self._dist_mode = dist_mode
    
        docs: List[Dict[str, Any]] = []
        for h in hits:
            payload = h.payload or {}
            text = self._extract_text_from_payload(payload)
    
            # ë©”íƒ€ë°ì´í„°: payload["metadata"] ìµœìš°ì„ , ì—†ìœ¼ë©´ payloadì—ì„œ doc ì œì™¸
            md = {}
            if isinstance(payload.get("metadata"), dict):
                md = dict(payload["metadata"])
            else:
                md = {k: v for k, v in payload.items() if k not in ("doc", "metadata")}
    
            raw = getattr(h, "score", None)  # QdrantëŠ” ë³´í†µ distanceë¥¼ scoreë¡œ ë°˜í™˜
            distance = float(raw) if raw is not None else None
            similarity = None
            if distance is not None and "cosine" in dist_mode:
                similarity = distance
    
            docs.append({
                "id": str(getattr(h, "id", "")),
                "content": text,            # âœ… ì´ì œ doc ê¸°ë°˜ ë³¸ë¬¸
                "metadata": md,             # âœ… metadata ê·¸ëŒ€ë¡œ
                "score": similarity if similarity is not None else (float(raw) if raw is not None else None),
                "distance": distance,
                "distance_mode": dist_mode,
            })
        return docs

    def rerank(self, question: str, docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        # ìë¦¬ë§Œë“¤ê¸°: í•„ìš”ì‹œ Vertex Rankingì´ë‚˜ cross-encoder ë¶™ì´ê¸°
        # ì§€ê¸ˆì€ ê·¸ëŒ€ë¡œ top_k ìƒìœ„ë§Œ ë¦¬í„´
        return (docs or [])[: self.top_k]

    def generate(self, question: str, docs: List[Dict[str, Any]]) -> str:
        if not docs:
            return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        ctx = "\n\n".join(d["content"] for d in docs)
        prompt = f"""
      ë‹¹ì‹ ì€ ì£¼ì‹ì‹œì¥ê³¼ ì—°ê¸ˆì— ì •í†µí•œ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.  
      ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê·¼ê±°ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë§ëŠ” í•œêµ­ì–´ ë‹µë³€ì„ ì¶©ì‹¤í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
      ë‹µë³€ì„ ì‘ì„± ì‹œ ì•„ë˜ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”.
      
        [ì‘ì„± ì§€ì¹¨]  
        1. ë‹µë³€ì€ **3ë‹¨ë½ ì´ìƒ**ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.  
        2. **ì¤‘ìš” í¬ì¸íŠ¸ëŠ” êµµê²Œ**, í•µì‹¬ ìˆ˜ì¹˜ëŠ” `ì½”ë“œë¸”ë¡ ìŠ¤íƒ€ì¼`ë¡œ í‘œì‹œí•˜ì„¸ìš”.  
        3. ë‹µë³€ ì¤‘ê°„ì—ëŠ” â–¸, âœ”, âœ¦ ê°™ì€ ë¶ˆë¦¿ ì•„ì´ì½˜ì„ í™œìš©í•´ ì‹œê°ì ìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•˜ì„¸ìš”.  
        4. ë§ˆì§€ë§‰ì— `---` êµ¬ë¶„ì„ ì„ ë„£ê³ , ê·¼ê±° ê¸°ì‚¬ í•œ ì¤„ ìš”ì•½ì„ ì²¨ë¶€í•˜ì„¸ìš”.  
        5. ëª¨í˜¸í•˜ê±°ë‚˜ ê·¼ê±° ì—†ëŠ” ë‚´ìš©ì€ ì“°ì§€ ë§ê³  "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.
        
        [ì»¨í…ìŠ¤íŠ¸]
        {ctx}
        
        [ì§ˆë¬¸]
        {question}
        """
        try:
            resp = self.gen_model.generate_content(prompt, generation_config={"temperature": 0.2})
            return (resp.text or "").strip()
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    def generate_stream(self, question: str, docs: List[Dict[str, Any]]) -> Generator[str, None, None]:
        """Gemini ìŠ¤íŠ¸ë¦¬ë° ì œë„ˆë ˆì´í„°: chunk ë¬¸ìì—´ì„ yield"""
        if not docs:
            yield "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            return

        ctx = "\n\n".join(d["content"] for d in docs)
        prompt = f"""
      ë‹¹ì‹ ì€ ì£¼ì‹ì‹œì¥ê³¼ ì—°ê¸ˆì— ì •í†µí•œ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.  
      ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê·¼ê±°ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ ì˜ë„ì— ë§ëŠ” í•œêµ­ì–´ ë‹µë³€ì„ ì¶©ì‹¤í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
      ë‹µë³€ì„ ì‘ì„± ì‹œ ì•„ë˜ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ì§€ì¼œì£¼ì„¸ìš”.
      
        [ì‘ì„± ì§€ì¹¨]  
        1. ë‹µë³€ì€ **3ë‹¨ë½ ì´ìƒ**ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.  
        2. **ì¤‘ìš” í¬ì¸íŠ¸ëŠ” êµµê²Œ**, í•µì‹¬ ìˆ˜ì¹˜ëŠ” `ì½”ë“œë¸”ë¡ ìŠ¤íƒ€ì¼`ë¡œ í‘œì‹œí•˜ì„¸ìš”.  
        3. ë‹µë³€ ì¤‘ê°„ì—ëŠ” â–¸, âœ”, âœ¦ ê°™ì€ ë¶ˆë¦¿ ì•„ì´ì½˜ì„ í™œìš©í•´ ì‹œê°ì ìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•˜ì„¸ìš”.  
        4. ë§ˆì§€ë§‰ì— `---` êµ¬ë¶„ì„ ì„ ë„£ê³ , ê·¼ê±° ê¸°ì‚¬ í•œ ì¤„ ìš”ì•½ì„ ì²¨ë¶€í•˜ì„¸ìš”.  
        5. ëª¨í˜¸í•˜ê±°ë‚˜ ê·¼ê±° ì—†ëŠ” ë‚´ìš©ì€ ì“°ì§€ ë§ê³  "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.

        [ì»¨í…ìŠ¤íŠ¸]
        {ctx}
        
        [ì§ˆë¬¸]
        {question}
        """

        try:
            responses = self.gen_model.generate_content(
                prompt,
                stream=True,
                generation_config={"temperature": 0.2},
            )

            # ì¼ë¶€ SDK ë²„ì „ì—ì„œ ì‘ë‹µì´ í›„ë³´/íŒŒì¸ ë¡œ ë‚˜ë‰˜ê±°ë‚˜ response.textì— ë¸íƒ€ê°€ ë“¤ì–´ì˜µë‹ˆë‹¤.
            for res in responses:
                # 1) ê°€ì¥ ê°„ë‹¨: ë¸íƒ€ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ
                if getattr(res, "text", None):
                    yield res.text
                    continue

                # 2) í›„ë³´/íŒŒì¸ ì—ì„œ í…ìŠ¤íŠ¸ ì¶•ì¶œ
                chunk = []
                for cand in (getattr(res, "candidates", None) or []):
                    if isinstance(cand, Candidate) and getattr(cand, "content", None):
                        for part in getattr(cand.content, "parts", []) or []:
                            t = getattr(part, "text", None)
                            if t:
                                chunk.append(t)
                if chunk:
                    yield "".join(chunk)

        except Exception as e:
            yield f"\n\n(ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e})"

    # ---------- public APIs ----------
    def answer(self, question: str) -> Dict[str, Any]:
        docs = self.retrieve(question)
        docs = self.rerank(question, docs) if self.use_rerank else docs[: self.top_k]
        ans = self.generate(question, docs)
        return {"answer": ans, "source_documents": docs}

    def answer_stream(self, question: str):
        """retrieve â†’ (ì˜µì…˜) rerank â†’ stream ìƒì„±ê¸°ë¥¼ ë°˜í™˜"""
        docs = self.retrieve(question)
        docs = self.rerank(question, docs) if self.use_rerank else docs[: self.top_k]
        return self.generate_stream(question, docs)
    

    def retrieve_only(self, question: str, top_k: int | None = None) -> List[Dict[str, Any]]:
        prev_top_k, self.top_k = self.top_k, (top_k or self.top_k)
        try:
            docs = self.retrieve(question)
            return docs[: (top_k or self.top_k)]
        finally:
            self.top_k = prev_top_k

    # ---------- ì§„ë‹¨í•¨ìˆ˜ ----------
    def diagnose(self) -> Dict[str, Any]:
        info: Dict[str, Any] = {}
    
        # 1) ì»¬ë ‰ì…˜/ë²¡í„° êµ¬ì„±
        try:
            col = self.qc.get_collection(self.collection)
            cfg = getattr(col, "config", None) or col
            vectors = getattr(cfg, "vectors", None)
            if hasattr(vectors, "size"):
                info["vector_size"] = vectors.size
                info["distance"] = str(getattr(vectors, "distance", ""))
                info["named_vectors"] = None
            elif isinstance(vectors, dict):
                info["named_vectors"] = {k: {"size": v.size, "distance": str(v.distance)} for k, v in vectors.items()}
            else:
                info["vectors_raw"] = str(vectors)
        except Exception as e:
            info["collection_error"] = f"{e}"
    
        # 2) í¬ì¸íŠ¸ ì¡´ì¬ì—¬ë¶€ + payload í‚¤ ë¯¸ë¦¬ë³´ê¸°
        try:
            scrolled = self.qc.scroll(
                collection_name=self.collection,
                limit=1,
                with_payload=True,
                with_vectors=False,
            )
            pts = getattr(scrolled, "points", None)
            if pts and len(pts) > 0:
                p = pts[0]
                payload = p.payload or {}
                info["has_points"] = True
                info["sample_payload_keys"] = list(payload.keys())
                # ë³¸ë¬¸ í›„ë³´ í‚¤ ëª‡ ê°œ ë¯¸ë¦¬ë³´ê¸°
                preview_keys = [k for k in ("doc", "page_content", "content", "text") if k in payload]
                preview = {}
                for k in preview_keys[:3]:
                    v = payload.get(k)
                    preview[k] = (v[:80] + "...") if isinstance(v, str) and len(v) > 80 else v
                info["payload_preview"] = preview
            else:
                info["has_points"] = False
        except Exception as e:
            info["scroll_error"] = f"{e}"
    
        # 3) ì¿¼ë¦¬ ì„ë² ë”© ì°¨ì›
        info["embed_dim_config"] = self.embed_dim
    
        # 4) í™˜ê²½ ë³€ìˆ˜/ì—”ë“œí¬ì¸íŠ¸
        info["qdrant_url"] = self.qdrant_url
        info["collection"] = self.collection
        info["vector_name"] = getattr(self, "vector_name", None)
    
        return info

