import os, threading
from typing import List, Dict, Any, Optional, TypedDict
import vertexai
from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput
from vertexai.generative_models import GenerativeModel
from qdrant_client import QdrantClient
from google.oauth2 import service_account
import streamlit as st  # Streamlit secrets ì‚¬ìš© ì‹œ


class RAGState(TypedDict):
    question: str
    documents: List[Dict[str, Any]]
    answer: Optional[str]


class NewsQnAService:
    """Qdrant + Gemini ê¸°ë°˜ RAG ì„œë¹„ìŠ¤"""
    _thread_local = threading.local()

    def __init__(
        self,
        project: str | None = None,
        location: str = "us-central1",
        qdrant_url: str | None = None,
        qdrant_key: str | None = None,
        collection: str = "stock_news",
        embed_model_name: str = "gemini-embedding-001",
        gen_model_name: str = "gemini-2.5-pro",
        embed_dim: int = 3072,
        top_k: int = 10,
        rerank_top_k: int = 5,
        use_rerank: bool = False,
    ):
        self.project = project or os.getenv("GOOGLE_CLOUD_PROJECT")
        self.location = location or os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
        if not self.project:
            raise RuntimeError("GOOGLE_CLOUD_PROJECT required")

        # ðŸ” Streamlit secrets â†’ ì„œë¹„ìŠ¤ê³„ì • í¬ë ˆë´ì…œ
        sa_info = None
        try:
            sa_info = st.secrets.get("gcp_service_account", None)
        except Exception:
            sa_info = None

        creds = None
        if sa_info:
            creds = service_account.Credentials.from_service_account_info(
                dict(sa_info),
                scopes=["https://www.googleapis.com/auth/cloud-platform"],
            )

        # âœ… Vertex ì´ˆê¸°í™” (credentials í¬í•¨)
        vertexai.init(project=self.project, location=self.location, credentials=creds)

        # Qdrant ì„¤ì •
        self.qdrant_url = qdrant_url or os.getenv("QDRANT_URL")
        self.qdrant_key = qdrant_key or os.getenv("QDRANT_API_KEY")
        if not (self.qdrant_url and self.qdrant_key):
            raise RuntimeError("QDRANT_URL / QDRANT_API_KEY required")

        self.collection = collection or os.getenv("COLLECTION_NAME", "stock_news")
        self.embed_model_name = embed_model_name or os.getenv("EMBED_MODEL_NAME", "gemini-embedding-001")
        self.gen_model_name = gen_model_name or os.getenv("GENAI_MODEL_NAME", "gemini-2.5-pro")
        self.embed_dim = int(embed_dim or int(os.getenv("EMBED_DIM", "3072")))
        self.top_k = int(top_k or int(os.getenv("DEFAULT_TOP_K", "10")))
        self.rerank_top_k = int(rerank_top_k or int(os.getenv("RERANK_TOP_K", "5")))
        self.use_rerank = use_rerank

        self.qc = QdrantClient(url=self.qdrant_url, api_key=self.qdrant_key)

        # ëª¨ë¸ì€ ìŠ¤ë ˆë“œ ë¡œì»¬ ìºì‹±
        self._ensure_models()

    # ---------- internals ----------
    def _ensure_models(self):
        if not hasattr(self._thread_local, "embed_model") or getattr(self._thread_local, "embed_name", None) != self.embed_model_name:
            self._thread_local.embed_model = TextEmbeddingModel.from_pretrained(self.embed_model_name)
            self._thread_local.embed_name = self.embed_model_name
        if not hasattr(self._thread_local, "gen_model") or getattr(self._thread_local, "gen_name", None) != self.gen_model_name:
            self._thread_local.gen_model = GenerativeModel(self.gen_model_name)
            self._thread_local.gen_name = self.gen_model_name

    @property
    def embed_model(self) -> TextEmbeddingModel:
        self._ensure_models()
        return self._thread_local.embed_model

    @property
    def gen_model(self) -> GenerativeModel:
        self._ensure_models()
        return self._thread_local.gen_model

    def _embed_query(self, text: str) -> list[float]:
        inp = [TextEmbeddingInput(text=text or "", task_type="RETRIEVAL_QUERY")]
        return self.embed_model.get_embeddings(inp, output_dimensionality=self.embed_dim)[0].values

    def _approx_token_len(self, s: str) -> int:
        # ëŒ€ëžµ 1 token ~= 4 chars ê°€ì •
        return max(1, len(s) // 4)

    def _clip_docs(self, docs: List[Dict[str, Any]], per_doc_chars: int = 1200, max_total_chars: int = 12000) -> List[Dict[str, Any]]:
        clipped: List[Dict[str, Any]] = []
        total = 0
        for d in docs:
            txt = (d.get("content") or "").strip()
            if not txt:
                continue
            if len(txt) > per_doc_chars:
                txt = txt[:per_doc_chars] + "â€¦"
            if total + len(txt) > max_total_chars:
                remain = max_total_chars - total
                if remain <= 0:
                    break
                txt = txt[:remain] + "â€¦"
            nd = dict(d)
            nd["content"] = txt
            clipped.append(nd)
            total += len(txt)
        return clipped

    def _make_prompt(self, question: str, ctx: str) -> str:
        return f"""
ë‹¹ì‹ ì€ ì£¼ì‹ì‹œìž¥ê³¼ ì—°ê¸ˆì— ì •í†µí•œ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ìž…ë‹ˆë‹¤.
ì•„ëž˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê·¼ê±°ë¡œ **í•œêµ­ì–´** ë‹µë³€ì„ ìž‘ì„±í•˜ì„¸ìš”.

[ìž‘ì„± ì§€ì¹¨]
1) (í˜„í™© ìš”ì•½) â†’ (ì›ì¸/ë§¥ë½) â†’ (ì „ë§/ì¡°ì–¸) ì˜ 3ë‹¨ë½ ì´ìƒ
2) ì¤‘ìš” í¬ì¸íŠ¸ëŠ” **êµµê²Œ**, í•µì‹¬ ìˆ˜ì¹˜ëŠ” `ì½”ë“œë¸”ë¡` ìŠ¤íƒ€ì¼
3) â–¸, âœ”, âœ¦ ë“±ì˜ ë¶ˆë¦¿ìœ¼ë¡œ ê°€ë…ì„± í–¥ìƒ
4) ë§ˆì§€ë§‰ì— --- ë„£ê³  ê·¼ê±° ê¸°ì‚¬ í•œ ì¤„ ìš”ì•½
5) ëª¨í˜¸/ê·¼ê±°ì—†ìŒ â†’ "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ëª…ì‹œ

[ì»¨í…ìŠ¤íŠ¸]
{ctx}

[ì§ˆë¬¸]
{question}
""".strip()

    # ---------- RAG steps ----------
    def retrieve(self, question: str) -> List[Dict[str, Any]]:
        qv = self._embed_query(question)
        hits = self.qc.search(
            collection_name=self.collection,
            query_vector=qv,
            limit=self.rerank_top_k if self.use_rerank else self.top_k,
            with_payload=True,
            with_vectors=False,
        )
        docs: List[Dict[str, Any]] = []
        for h in hits:
            payload = h.payload or {}
            docs.append({
                "id": str(h.id),
                "content": payload.get("text", ""),
                "metadata": {k: v for k, v in payload.items() if k != "text"},
                "score": float(getattr(h, "score", 1.0)),
            })
        return docs

    def rerank(self, question: str, docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        # TODO: í•„ìš” ì‹œ cross-encoder/Vertex Ranking ì—°ê²°
        return (docs or [])[: self.top_k]

    def generate(self, question: str, docs: List[Dict[str, Any]]) -> str:
        if not docs:
            return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # 1) ì»¨í…ìŠ¤íŠ¸ í´ë¦¬í•‘ (ë¬¸ì„œë‹¹/ì „ì²´ ê¸¸ì´ ì œí•œ)
        docs_small = self._clip_docs(docs, per_doc_chars=1200, max_total_chars=12000)
        ctx = "\n\n".join(d["content"] for d in docs_small)
        prompt = self._make_prompt(question, ctx)

        # 2) í† í° ì˜ˆì‚° ë³´í˜¸: í”„ë¡¬í”„íŠ¸ 6k í† í°(â‰ˆ24k chars) ì´í•˜
        MAX_PROMPT_TOKENS = 6000
        MAX_PROMPT_CHARS = MAX_PROMPT_TOKENS * 4
        if len(prompt) > MAX_PROMPT_CHARS:
            over = len(prompt) - MAX_PROMPT_CHARS
            keep = max(0, len(ctx) - over - 1000)  # ì§€ì¹¨ ì—¬ìœ 
            ctx = (ctx[:keep] + "â€¦") if keep > 0 else ""
            prompt = self._make_prompt(question, ctx)

        gcfg = {"temperature": 0.2, "max_output_tokens": 800}

        def _call_model(p: str) -> str:
            resp = self.gen_model.generate_content(p, generation_config=gcfg)
            # 1) ì¼ë°˜ ê²½ë¡œ
            text = getattr(resp, "text", None)
            if text:
                return text.strip()
            # 2) candidates â†’ parts ê²½ë¡œ
            cands = getattr(resp, "candidates", None)
            if cands and getattr(cands[0], "content", None):
                parts = getattr(cands[0].content, "parts", None)
                if parts and hasattr(parts[0], "text"):
                    return (parts[0].text or "").strip()
            # 3) ì‹¤íŒ¨
            raise RuntimeError("empty_text")

        # 3) 1ì°¨ í˜¸ì¶œ
        try:
            return _call_model(prompt)
        except Exception:
            # 4) ìž¬ì‹œë„: ë” ì§§ì€ ì»¨í…ìŠ¤íŠ¸
            docs_tiny = self._clip_docs(docs, per_doc_chars=600, max_total_chars=6000)
            ctx2 = "\n\n".join(d["content"] for d in docs_tiny)
            prompt2 = self._make_prompt(question, ctx2)
            try:
                return _call_model(prompt2)
            except Exception:
                # 5) ìµœì¢… ì‹¤íŒ¨
                return (
                    "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\n"
                    "ì‚¬ìœ : ëª¨ë¸ ì•ˆì „/ê¸¸ì´ ì œí•œìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. "
                    "ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ì£¼ì‹œê±°ë‚˜(ì˜ˆ: ê¸°ê°„/ì¢…ëª©/ì´ìŠˆ), ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¤„ì—¬ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."
                )

    # ---------- public APIs ----------
    def answer(self, question: str) -> Dict[str, Any]:
        docs = self.retrieve(question)
        docs = self.rerank(question, docs) if self.use_rerank else docs[: self.top_k]
        ans = self.generate(question, docs)
        return {"answer": ans, "source_documents": docs}

    def retrieve_only(self, question: str, top_k: int | None = None) -> List[Dict[str, Any]]:
        prev_top_k, self.top_k = self.top_k, (top_k or self.top_k)
        try:
            docs = self.retrieve(question)
            return docs[: (top_k or self.top_k)]
        finally:
            self.top_k = prev_top_k
