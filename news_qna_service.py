import os, re, threading
from typing import List, Dict, Any, Optional, TypedDict
import vertexai
from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput
from vertexai.generative_models import GenerativeModel
from qdrant_client import QdrantClient
from google.oauth2 import service_account
import streamlit as st  # Streamlit secrets ì‚¬ìš© ì‹œ


class RAGState(TypedDict):
    question: str
    documents: List[Dict[str, Any]]
    answer: Optional[str]

class NewsQnAService:
    """Qdrant + Gemini ê¸°ë°˜ RAG ì„œë¹„ìŠ¤"""
    _thread_local = threading.local()

    def __init__(
        self,
        project: str | None = None,
        location: str = "us-central1",
        qdrant_url: str | None = None,
        qdrant_key: str | None = None,
        collection: str = "stock_news",
        embed_model_name: str = "gemini-embedding-001",
        gen_model_name: str = "gemini-2.5-pro",
        embed_dim: int = 3072,
        top_k: int = 5,
        rerank_top_k: int = 5,
        use_rerank: bool = False,
    ):
        self.project = project or os.getenv("GOOGLE_CLOUD_PROJECT")
        self.location = location or os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
        if not self.project:
            raise RuntimeError("GOOGLE_CLOUD_PROJECT required")

        # ðŸ” Streamlit secretsì—ì„œ ì„œë¹„ìŠ¤ê³„ì • ì½ì–´ credentials ìƒì„±
        sa_info = None
        try:
            sa_info = st.secrets.get("gcp_service_account", None)
        except Exception:
            sa_info = None

        creds = None
        if sa_info:
            creds = service_account.Credentials.from_service_account_info(
                dict(sa_info),
                scopes=["https://www.googleapis.com/auth/cloud-platform"],
            )

        # âœ… credentialsê¹Œì§€ ëª…ì‹œí•´ì„œ ì´ˆê¸°í™”

        vertexai.init(project=self.project, location=self.location,credentials=creds)

        self.qdrant_url = qdrant_url or os.getenv("QDRANT_URL")
        self.qdrant_key = qdrant_key or os.getenv("QDRANT_API_KEY")
        if not (self.qdrant_url and self.qdrant_key):
            raise RuntimeError("QDRANT_URL / QDRANT_API_KEY required")

        self.collection = collection or os.getenv("COLLECTION_NAME", "stock_news")
        self.embed_model_name = embed_model_name or os.getenv("EMBED_MODEL_NAME", "gemini-embedding-001")
        self.gen_model_name = gen_model_name or os.getenv("GENAI_MODEL_NAME", "gemini-2.5-pro")
        self.embed_dim = int(embed_dim or int(os.getenv("EMBED_DIM", "3072")))
        self.top_k = int(top_k or int(os.getenv("DEFAULT_TOP_K", "5")))
        self.rerank_top_k = int(rerank_top_k or int(os.getenv("RERANK_TOP_K", "5")))
        self.use_rerank = use_rerank

        self.qc = QdrantClient(url=self.qdrant_url, api_key=self.qdrant_key)

        # ëª¨ë¸ì€ ìŠ¤ë ˆë“œ ë¡œì»¬ ìºì‹±
        self._ensure_models()

    # ---------- internals ----------
    def _ensure_models(self):
        if not hasattr(self._thread_local, "embed_model") or getattr(self._thread_local, "embed_name", None) != self.embed_model_name:
            self._thread_local.embed_model = TextEmbeddingModel.from_pretrained(self.embed_model_name)
            self._thread_local.embed_name = self.embed_model_name
        if not hasattr(self._thread_local, "gen_model") or getattr(self._thread_local, "gen_name", None) != self.gen_model_name:
            self._thread_local.gen_model = GenerativeModel(self.gen_model_name)
            self._thread_local.gen_name = self.gen_model_name

    @property
    def embed_model(self) -> TextEmbeddingModel:
        self._ensure_models()
        return self._thread_local.embed_model

    @property
    def gen_model(self) -> GenerativeModel:
        self._ensure_models()
        return self._thread_local.gen_model

    def _embed_query(self, text: str) -> list[float]:
        inp = [TextEmbeddingInput(text=text or "", task_type="RETRIEVAL_QUERY")]
        return self.embed_model.get_embeddings(inp, output_dimensionality=self.embed_dim)[0].values

    # ---------- RAG steps ----------
    def retrieve(self, question: str) -> List[Dict[str, Any]]:
        qv = self._embed_query(question)
        hits = self.qc.search(
            collection_name=self.collection,
            query_vector=qv,
            limit=self.top_k if not self.use_rerank else self.rerank_top_k,
            with_payload=True,
            with_vectors=False,
        )
        docs: List[Dict[str, Any]] = []
        for h in hits:
            payload = h.payload or {}
            docs.append({
                "id": str(h.id),
                "content": payload.get("text", ""),
                "metadata": {k: v for k, v in payload.items() if k != "text"},
                "score": float(getattr(h, "score", 1.0)),
            })
        return docs

    def rerank(self, question: str, docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        # ìžë¦¬ë§Œë“¤ê¸°: í•„ìš”ì‹œ Vertex Rankingì´ë‚˜ cross-encoder ë¶™ì´ê¸°
        # ì§€ê¸ˆì€ ê·¸ëŒ€ë¡œ top_k ìƒìœ„ë§Œ ë¦¬í„´
        return (docs or [])[: self.top_k]

    def generate(self, question: str, docs: List[Dict[str, Any]]) -> str:
        if not docs:
            return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        ctx = "\n\n".join(d["content"] for d in docs)
        prompt = f"""
      ë‹¹ì‹ ì€ ì£¼ì‹ì‹œìž¥ê³¼ ì—°ê¸ˆì— ì •í†µí•œ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ìž…ë‹ˆë‹¤.  
      ì•„ëž˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê·¼ê±°ë¡œ í•œêµ­ì–´ ë‹µë³€ì„ ìž‘ì„±í•˜ì„¸ìš”.  
          ë‹¹ì‹ ì€ ì£¼ì‹ì‹œìž¥ê³¼ ì—°ê¸ˆì— ì •í†µí•œ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ìž…ë‹ˆë‹¤.  
          ì•„ëž˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê·¼ê±°ë¡œ í•œêµ­ì–´ ë‹µë³€ì„ ìž‘ì„±í•˜ì„¸ìš”.  
      
        [ìž‘ì„± ì§€ì¹¨]  
        1. ë‹µë³€ì€ **3ë‹¨ë½ ì´ìƒ**ìœ¼ë¡œ êµ¬ì„±í•˜ì„¸ìš”.  
           - (1) í˜„í™© ìš”ì•½  
           - (2) ì›ì¸/ë§¥ë½ ë¶„ì„  
           - (3) í–¥í›„ ì „ë§ ë° íˆ¬ìžìž ì¡°ì–¸  
        2. **ì¤‘ìš” í¬ì¸íŠ¸ëŠ” êµµê²Œ**, í•µì‹¬ ìˆ˜ì¹˜ëŠ” `ì½”ë“œë¸”ë¡ ìŠ¤íƒ€ì¼`ë¡œ í‘œì‹œí•˜ì„¸ìš”.  
        3. ë‹µë³€ ì¤‘ê°„ì—ëŠ” â–¸, âœ”, âœ¦ ê°™ì€ ë¶ˆë¦¿ ì•„ì´ì½˜ì„ í™œìš©í•´ ì‹œê°ì ìœ¼ë¡œ ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•˜ì„¸ìš”.  
        4. ë§ˆì§€ë§‰ì— `---` êµ¬ë¶„ì„ ì„ ë„£ê³ , ê·¼ê±° ê¸°ì‚¬ í•œ ì¤„ ìš”ì•½ì„ ì²¨ë¶€í•˜ì„¸ìš”.  
        5. ëª¨í˜¸í•˜ê±°ë‚˜ ê·¼ê±° ì—†ëŠ” ë‚´ìš©ì€ ì“°ì§€ ë§ê³  "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µí•˜ì„¸ìš”.

        [ë‹µë³€ì˜ˆì‹œ]
        ðŸ“Š í˜„í™© ìš”ì•½

        ìµœê·¼ ì—”ë¹„ë””ì•„ ì£¼ê°€ê°€ 30ì¼ ì¢…ê°€ ê¸°ì¤€ 100ì¼ ì´ë™í‰ê· ì„  ì•„ëž˜ë¡œ í•˜ë½í•˜ë©´ì„œ ê¸°ìˆ ì  ì§€í‘œìƒ ë¶€ì •ì ì¸ ì‹ í˜¸ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. íŠ¹ížˆ ë³€ë™ì„±ì´ ë¹„íŠ¸ì½”ì¸ë³´ë‹¤ë„ 2ë°° ì´ìƒ ë†’ì•„ì¡Œë‹¤ëŠ” ì ì´ íˆ¬ìžìž ë¶ˆì•ˆ ì‹¬ë¦¬ë¥¼ í‚¤ìš°ê³  ìžˆìŠµë‹ˆë‹¤.

        ðŸ” ì›ì¸ ë° ë°°ê²½
        
        â–¸ ìµœê·¼ 2ì£¼ê°„ ë¹…í…Œí¬ ì „ë°˜ì˜ ì•½ì„¸ê°€ ë™ë°˜ë˜ë©° ì—”ë¹„ë””ì•„ ì£¼ê°€ì— ë¶€ë‹´ì´ ë˜ì—ˆê³ ,
        â–¸ ê¸ˆë¦¬ ì¸í•˜ ì‹œ ìˆ˜í˜œì£¼ì— ëŒ€í•œ ê´€ì‹¬ì´ ë¶„ì‚°ëœ ê²ƒë„ ì¶”ê°€ì ì¸ í•˜ë½ ì••ë ¥ìœ¼ë¡œ ìž‘ìš©í–ˆìŠµë‹ˆë‹¤.
        ê·¸ëŸ¬ë‚˜ ì¼ë¶€ ì „ë¬¸ê°€ë“¤ì€ ì´ë²ˆ ì¡°ì •ì´ ìž¥ê¸° ì„±ìž¥ì„±ì—ëŠ” í° ì˜í–¥ì„ ì£¼ì§€ ì•Šì„ ê²ƒì´ë¼ ê°•ì¡°í•˜ê³  ìžˆìŠµë‹ˆë‹¤.

        ðŸ“ˆ í–¥í›„ ì „ë§ & íˆ¬ìžìž ì¡°ì–¸

        âœ” ë‹¨ê¸°ì ìœ¼ë¡œëŠ” ì¶”ê°€ í•˜ë½ ê°€ëŠ¥ì„±ì„ ì—¼ë‘ì— ë‘ì–´ì•¼ í•˜ë©°, ë³´ìˆ˜ì  íˆ¬ìžìžë¼ë©´ ê´€ë§ì´ ìœ ë¦¬í•©ë‹ˆë‹¤.
        âœ” ë°˜ë©´, ìž¥ê¸°ì  ê´€ì ì—ì„œëŠ” ë§¤ìˆ˜ ê¸°íšŒë¡œ ìž‘ìš©í•  ìˆ˜ ìžˆë‹¤ëŠ” ì ì—ì„œ ê³µê²©ì  íˆ¬ìžìžì—ê²ŒëŠ” ê¸ì •ì ì¼ ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
        âœ¦ ë”°ë¼ì„œ ë¦¬ìŠ¤í¬ ê´€ë¦¬ì™€ ë¶„í•  ë§¤ìˆ˜ ì „ëžµì´ ê· í˜• ìž¡ížŒ ì ‘ê·¼ë²•ì´ ë  ê²ƒìž…ë‹ˆë‹¤.

        ðŸ“° ê·¼ê±° ê¸°ì‚¬: ì—”ë¹„ë””ì•„ ì£¼ê°€ê°€ 30ì¼ ì¢…ê°€ ê¸°ì¤€ 100ì¼ ì´ë™í‰ê· ì„  ì•„ëž˜ë¡œ ë–¨ì–´ì§€ë©° ê¸°ìˆ ì  ë¶€ì • ì‹ í˜¸ ë°œìƒ
        
        [ì»¨í…ìŠ¤íŠ¸]
        {ctx}
        
        [ì§ˆë¬¸]
        {question}
        """
        try:
            resp = self.gen_model.generate_content(prompt, generation_config={"temperature": 0.2, "max_output_tokens": 1200})
            return (resp.text or "").strip()
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    # ---------- public APIs ----------
    def answer(self, question: str) -> Dict[str, Any]:
        docs = self.retrieve(question)
        docs = self.rerank(question, docs) if self.use_rerank else docs[: self.top_k]
        ans = self.generate(question, docs)
        return {"answer": ans, "source_documents": docs}

    def retrieve_only(self, question: str, top_k: int | None = None) -> List[Dict[str, Any]]:
        prev_top_k, self.top_k = self.top_k, (top_k or self.top_k)
        try:
            docs = self.retrieve(question)
            return docs[: (top_k or self.top_k)]
        finally:
            self.top_k = prev_top_k
