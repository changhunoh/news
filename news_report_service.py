# news_report_service.py
# ------------------------------------------------------------
# Qdrant + Vertex AI (Gemini / Embedding) ê¸°ë°˜ RAG ì„œë¹„ìŠ¤
#   - stockìœ¼ë¡œ ë¨¼ì € ì„œë²„-ì‚¬ì´ë“œ í•„í„° â†’ í•„í„°ëœ ì„œë¸Œì…‹ì—ì„œ ë²¡í„°ê²€ìƒ‰
#   - ë‹¨ì¼ ì§ˆë¬¸ + 5ê°œ ì¢…ëª© ë³‘ë ¬(Map) â†’ ìµœì¢… í†µí•©(Reduce)
# ------------------------------------------------------------

import os, threading, re
from typing import List, Dict, Any, Optional, TypedDict
from concurrent.futures import ThreadPoolExecutor, as_completed

import vertexai
from vertexai.language_models import TextEmbeddingModel, TextEmbeddingInput
from vertexai.generative_models import GenerativeModel, SafetySetting, HarmCategory, HarmBlockThreshold, GenerationConfig

from qdrant_client import QdrantClient
from qdrant_client.models import (
    Filter, FieldCondition, MatchValue, PayloadSchemaType
)
from dotenv import load_dotenv
from datetime import datetime, date, timedelta


load_dotenv()
# Streamlitì´ ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë‹ˆ ì•ˆì „ import
try:
    import streamlit as st
except Exception:
    class _DummySt:
        secrets = {}
    st = _DummySt()

# ì„œë¹„ìŠ¤ê³„ì • import (í™˜ê²½ì— ë”°ë¼ ì—†ì„ ìˆ˜ ìˆìŒ)
try:
    from google.oauth2 import service_account
except Exception:
    service_account = None


class RAGState(TypedDict):
    question: str
    documents: List[Dict[str, Any]]
    answer: Optional[str]


class NewsReportService:
    """ë£¨íŠ¸ payload ìŠ¤í‚¤ë§ˆ(text/stock/...) ê¸°ì¤€. stock pre-filter â†’ ë²¡í„°ê²€ìƒ‰."""
    #_thread_local = threading.local()

    def __init__(
        self,
        project: Optional[str] = None,
        location: str = "us-central1",
        qdrant_url: Optional[str] = None,
        qdrant_key: Optional[str] = None,
        collection: str = "stock_news",
        embed_model_name: str = "gemini-embedding-001",
        gen_model_name: str = "gemini-2.5-pro", #ìµœì¢… ë¦¬í¬íŠ¸ ëª¨ë¸
        rag_model_name: str = "gemini-2.5-flash-lite", # RAG ëª¨ë¸
        embed_dim: int = 3072,
        top_k: int = 1,
        rerank_top_k: int = 1,
        use_rerank: bool = False,
    ):
        # ---- GCP & Vertex init (ë³´ì•ˆ ê´€ë ¨ ì„¤ì •) ----

        self.project = project or os.getenv("GOOGLE_CLOUD_PROJECT")
        self.location = location or os.getenv("GOOGLE_CLOUD_LOCATION", "us-central1")
        if not self.project:
            raise RuntimeError("GOOGLE_CLOUD_PROJECT required")
        sa_info = None
        try:
            sa_info = getattr(st, "secrets", {}).get("gcp_service_account", None)  # type: ignore[attr-defined]
        except Exception:
            sa_info = None

        creds = None
        if sa_info and service_account is not None:
            creds = service_account.Credentials.from_service_account_info(
                dict(sa_info),
                scopes=["https://www.googleapis.com/auth/cloud-platform"],
            )

        vertexai.init(project=self.project, location=self.location, credentials=creds)

        # ---- Qdrant (ë°±í„° DB ì„¤ì •) ----
        self.qdrant_url = qdrant_url or os.getenv("QDRANT_URL")
        self.qdrant_key = qdrant_key or os.getenv("QDRANT_API_KEY")
        if not (self.qdrant_url and self.qdrant_key):
            raise RuntimeError("QDRANT_URL / QDRANT_API_KEY required")
        # ---- Model Config  ----
        self.collection = collection or os.getenv("COLLECTION_NAME", "stock_news")
        self.embed_model_name = embed_model_name or os.getenv("EMBED_MODEL_NAME", "gemini-embedding-001")
        self.gen_model_name = gen_model_name or os.getenv("GENAI_MODEL_NAME", "gemini-2.5-pro")
        self.rag_model_name = rag_model_name or os.getenv("RAG_MODEL_NAME",'gemini-2.5-flash-lite')
        self.embed_dim = int(embed_dim or int(os.getenv("EMBED_DIM", "3072")))
        self.top_k = int(top_k or int(os.getenv("DEFAULT_TOP_K", "1")))
        self.rerank_top_k = int(rerank_top_k or int(os.getenv("RERANK_TOP_K", "1")))
        self.use_rerank = use_rerank
        
        self._dist_mode: Optional[str] = None

        # Qdrant ë²¡í„°DB 
        self.qc = QdrantClient(url=self.qdrant_url, api_key=self.qdrant_key)
        # self.gen_model_name = gen_model_name
        # self.rag_model_name = rag_model_name

        # ëª¨ë¸ í•¸ë“¤ ì¤€ë¹„
        # self._ensure_models()
    
        # ëª¨ë¸ ê³µìœ  í•¸ë“¤
        self.embed_model = TextEmbeddingModel.from_pretrained(self.embed_model_name)
        self.rag_model   = GenerativeModel(self.rag_model_name)
        self.gen_model   = GenerativeModel(self.gen_model_name)
        # í•„í„°ë¥¼ ì“°ë ¤ë©´ ì¸ë±ìŠ¤ê°€ í•„ìš” â†’ í•œ ë²ˆ ë³´ì¥
        self._ensure_stock_index()

    # ----------------- ë‚´ë¶€ ìœ í‹¸ -----------------
    # def _ensure_models(self):
    #     if (not hasattr(self._thread_local, "embed_model")
    #         or getattr(self._thread_local, "embed_name", None) != self.embed_model_name):
    #         self._thread_local.embed_model = TextEmbeddingModel.from_pretrained(self.embed_model_name)
    #         self._thread_local.embed_name = self.embed_model_name

    #     if not hasattr(self._thread_local, "rag_model") or getattr(self._thread_local, "rag_model_name", None) != self.rag_model_name:
    #         self._thread_local.rag_model = GenerativeModel(self.rag_model_name)
    #         self._thread_local.rag_model_name = self.rag_model_name

    #     if (not hasattr(self._thread_local, "gen_model")
    #         or getattr(self._thread_local, "gen_name", None) != self.gen_model_name):
    #         self._thread_local.gen_model = GenerativeModel(self.gen_model_name)
    #         self._thread_local.gen_name = self.gen_model_name

    # def _tl_qc(self) -> QdrantClient:
    #     if not hasattr(self._thread_local, "qc"):
    #         self._thread_local.qc = QdrantClient(url=self.qdrant_url, api_key=self.qdrant_key)
    #     return m if m is not None else self._embed_model_shared

    # @property
    # def embed_model(self) -> TextEmbeddingModel:
    #     self._ensure_models()
    #     m = getattr(self._thread_local, "embed_model", None)
    #     return self._thread_local.embed_model

    # @property
    # def rag_model(self) -> GenerativeModel:
    #     self._ensure_models()
    #     m = getattr(self._thread_local, "rag_model", None)
    #     return m if m is not None else self._rag_model_shared
    
    # @property
    # def gen_model(self) -> GenerativeModel:
    #     self._ensure_models()
    #     m = getattr(self._thread_local, "gen_model", None)
    #     return m if m is not None else self._gen_model_shared

    today = datetime.now().strftime("%Y-%m-%d")
    
    def _ensure_stock_index(self) -> None:
        """ë£¨íŠ¸ 'stock'ì— keyword ì¸ë±ìŠ¤ ë³´ì¥(ì—†ìœ¼ë©´ ìƒì„±)."""
        try:
            self.qc.create_payload_index(
                collection_name=self.collection,
                field_name="stock",
                field_schema=PayloadSchemaType.KEYWORD,  # ë˜ëŠ” "keyword"
                wait=True,
            )
        except Exception:
            # ì´ë¯¸ ìˆê±°ë‚˜ ê¶Œí•œ ë¬¸ì œë©´ ì¡°ìš©íˆ íŒ¨ìŠ¤
            pass

    # ----------------- ì„ë² ë”© & í…ìŠ¤íŠ¸ -----------------
    def _embed_query(self, text: str) -> List[float]:
        inp = [TextEmbeddingInput(text=text or "", task_type="RETRIEVAL_QUERY")]
        return self.embed_model.get_embeddings(inp, output_dimensionality=self.embed_dim)[0].values

    @staticmethod
    def _extract_text_from_payload(payload: dict) -> str:
        """í˜„ì¬ ìŠ¤í‚¤ë§ˆ: ë³¸ë¬¸ì€ payload['text'] (metadata ë˜í¼ ì—†ìŒ)"""
        if not isinstance(payload, dict):
            return ""
        if isinstance(payload.get("text"), str):
            return payload["text"]
        # í˜¸í™˜(ì´ì „ ìŠ¤í‚¤ë§ˆ)
        doc = payload.get("doc")
        if isinstance(doc, str):
            return doc
        if isinstance(doc, dict):
            return doc.get("content") or doc.get("text") or doc.get("page_content") or ""
        return ""

    # ----------------- Retrieve (stock pre-filter â†’ vector search) -----------------
    def retrieve(self, question: str, stock: Optional[str] = None) -> List[Dict[str, Any]]:
        qv = self._embed_query(question)
        want = self.rerank_top_k if self.use_rerank else self.top_k
        q_filter = None
        if stock:
            self._ensure_stock_index()
            q_filter = Filter(must=[FieldCondition(key="stock", match=MatchValue(value=str(stock)))])

        # í•„í„°ëœ ì„œë¸Œì…‹ì—ì„œ ë²¡í„°ê²€ìƒ‰
        try:
            hits = self.qc.search(
                collection_name=self.collection,
                query_vector=qv,
                limit=want,
                with_payload=True,
                with_vectors=False,
                query_filter=q_filter,
            )
        except Exception as e:
            # ì¸ë±ìŠ¤ ì´ìŠˆ ë“± â†’ í•œ ë²ˆ ë” ì¸ë±ìŠ¤ ë³´ì¥ í›„ ì¬ì‹œë„
            if stock and ("Index required" in str(e) or "Bad request" in str(e)):
                self._ensure_stock_index()
                hits = self.qc.search(
                    collection_name=self.collection,
                    query_vector=qv,
                    limit=want,
                    with_payload=True,
                    with_vectors=False,
                    query_filter=q_filter,
                )
            else:
                raise

        # distance ëª¨ë“œ ìºì‹œ
        if self._dist_mode is None:
            try:
                info = self.qc.get_collection(self.collection)
                params = getattr(info.config, "params", None) or getattr(info, "config", None)
                vectors = getattr(params, "vectors", None)
                self._dist_mode = str(getattr(vectors, "distance", "")).lower() if vectors else ""
            except Exception:
                self._dist_mode = ""

        # hits â†’ docs
        docs: List[Dict[str, Any]] = []
        for h in hits:
            payload = h.payload or {}
            text = self._extract_text_from_payload(payload)
            md = {k: v for k, v in payload.items() if k != "text"}  # ë£¨íŠ¸ payload ì „ì²´ë¥¼ ë©”íƒ€ë°ì´í„°ë¡œ
            raw = getattr(h, "score", None)
            distance = float(raw) if raw is not None else None
            docs.append({
                "id": str(getattr(h, "id", "")),
                "content": text,
                "metadata": md,
                "score": distance,
                "distance": distance,
                "distance_mode": self._dist_mode,
            })
        return docs

    # ----------------- (ì„ íƒ) ë¦¬ë­í¬ -----------------
    def rerank(self, question: str, docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        return (docs or [])[: self.top_k]
    
    def _safe_settings(self):
        return [
            SafetySetting(category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
                          threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,
                          threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
                          threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HARASSMENT,
                          threshold=HarmBlockThreshold.BLOCK_NONE),
        ]    
    def _gen_config(self, temperature: float = 0.2) -> GenerationConfig:
        # í•„ìš” ì‹œ ì—¬ê¸°ì„œ top_p/top_k ë„ ì¡°ì ˆ ê°€ëŠ¥
        return GenerationConfig(temperature=temperature)

    def _extract_text(self, resp) -> str:
        # vertexai SDK ì‘ë‹µì„ ìµœëŒ€í•œ ì•ˆì „í•˜ê²Œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        try:
            if getattr(resp, "text", None):
                return resp.text.strip()
            cands = getattr(resp, "candidates", None) or []
            if not cands:
                return ""
            first = cands[0]
            content = getattr(first, "content", None)
            parts = getattr(content, "parts", None) or []
            out = []
            for p in parts:
                t = getattr(p, "text", None)
                if t:
                    out.append(t)
            return "\n".join(out).strip()
        except Exception:
            return ""
    
    # ----------------- Generate (Rag ê¸°ëŠ¥ ìˆ˜í–‰) -----------------
    def generate(self, question: str, docs: List[Dict[str, Any]], stock: Optional[str] = None) -> str:
        #self._ensure_models()
        if not docs:
            return "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        def _trunc(s: str, limit=1600):
            s = s or ""
            return s if len(s) <= limit else s[:limit] + "..."
        ctx = "\n\n---\n\n".join(_trunc(d["content"]) for d in docs[:5])

        prompt = f"""
ë‹¹ì‹ ì€ ì£¼ì‹ì‹œì¥ê³¼ ì—°ê¸ˆì— ì •í†µí•œ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ {stock} ì¢…ëª©ì˜ ê°€ê²© ê²°ì •ì— ì¤‘ìš”í•œ í•µì‹¬ì •ë³´ë¥¼ ìš”ì•½í•˜ê³ ,
ì¢…ëª©ì˜ ì „ë§ì— ëŒ€í•œ ğŸ˜Šê¸ì • ë˜ëŠ” ğŸ˜¥ë¶€ì •ì„ íŒë‹¨í•´ì£¼ì„¸ìš”.
ë¬¸ë‹¨ 2~3ê°œ, ì „ì²´ 350~450ë‹¨ì–´ ë‚´, ìˆ˜ì¹˜ëŠ” `ë°±í‹±`ìœ¼ë¡œ í‘œì‹œí•´ì£¼ì„¸ìš”.

[ì‘ì„± ì§€ì¹¨]
1) ë‹µë³€ì€ 3ë‹¨ë½ ì´ìƒ
2) ê·¼ê±° ì—†ëŠ” ë‚´ìš©ì€ ì“°ì§€ ë§ ê²ƒ(ëª¨í˜¸í•˜ë©´ 'ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
3) ğŸ˜Šê¸ì • ë˜ëŠ” ğŸ˜¥ë¶€ì •ì— ëŒ€í•œ íŒë‹¨ í›„ í•µì‹¬ì •ë³´ ìš”ì•½ ì œê³µ

[ëŒ€ìƒ ì¢…ëª©]
{stock}

[ì»¨í…ìŠ¤íŠ¸ ë°œì·Œ]
{ctx}

[ì§ˆë¬¸]
{question}
"""
        try:
            # rag_model (2.5 flahs light) ì‚¬ìš©
            resp = self.rag_model.generate_content(
                prompt,
                generation_config=self._gen_config(temperature=0.0),
                safety_settings=self._safe_settings(),
            )
            text = self._extract_text(resp)
            if not text:
                return "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì•ˆì „í•„í„° ë˜ëŠ” í† í° í•œë„ì— ì˜í•´ ì‘ë‹µì´ ë¹„ì—ˆìŠµë‹ˆë‹¤."
            return text
        except Exception as e:
            return f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    # ----------------- Public APIs -----------------
    def answer(self, question: str, stock: Optional[str] = None) -> Dict[str, Any]:
        docs = self.retrieve(question, stock=stock)
        docs = self.rerank(question, docs) if self.use_rerank else docs[: self.top_k]
        ans = self.generate(question, docs, stock=stock)
        return {"answer": ans, "source_documents": docs}

    def retrieve_only(self, question: str, top_k: Optional[int] = None, stock: Optional[str] = None) -> List[Dict[str, Any]]:
        prev_top_k, self.top_k = self.top_k, (top_k or self.top_k)
        try:
            docs = self.retrieve(question, stock=stock)
            return docs[: (top_k or self.top_k)]
        finally:
            self.top_k = prev_top_k

    # ----------------- ë‹¤ì¤‘ ì¢…ëª© Map â†’ Reduce -----------------
    def _stock_question(self, stock: str, template: Optional[str] = None) -> str:
        template = template or "{stock} ê´€ë ¨í•´ì„œ ì¢…ëª©ì˜ ê°€ê²©ì— ì¤‘ìš”í•œ ë‰´ìŠ¤ëŠ”?"
        return template.format(stock=stock)

    def answer_for_stock(self, stock: str, template: Optional[str] = None) -> Dict[str, Any]:
        q = self._stock_question(stock, template)
        res = self.answer(q, stock=stock)
        return {
            "stock": stock,
            "question": q,
            "answer": res["answer"],
            "source_documents": res.get("source_documents", []),
        }

    def answer_multi_stocks(self, stocks: List[str], template: Optional[str] = None, max_workers: int = 5) -> List[Dict[str, Any]]:
        results: List[Optional[Dict[str, Any]]] = [None] * len(stocks)
        def _one(i: int, s: str) -> tuple[int, Dict[str, Any]]:
            return i, self.answer_for_stock(s, template=template)
        with ThreadPoolExecutor(max_workers=max_workers) as ex:
            futs = [ex.submit(_one, i, s) for i, s in enumerate(stocks)]
            for fut in as_completed(futs):
                i, r = fut.result()
                results[i] = r
        return [r for r in results if r is not None]

    def _reduce_across_stocks(self, template , per_stock_results: List[Dict[str, Any]]) -> str:
        #self._ensure_models()
        if not per_stock_results:
            return "ì¢…ëª©ë³„ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤."
        
        def _fmt_sources(docs: List[Dict[str, Any]]) -> List[str]:
            out = []
            for d in (docs or [])[:3]:
                md = d.get("metadata", {}) or {}
                title = md.get("title") or md.get("headline") or md.get("doc_title") or md.get("doc_id") or ""
                url = md.get("url") or md.get("link") or md.get("source_url") or ""
                if title and url: out.append(f"{title} â€” {url}")
                elif title:       out.append(title)
                elif url:         out.append(url)
            return out
    
        def _hard_trunc(s: str, limit=1200):
            s = (s or "").strip()
            return s if len(s) <= limit else s[:limit] + "..."
    
        lines, source_lines = [], []
        for r in per_stock_results:
            stock = r.get("stock","")
            ans = r.get("answer") or ""
            ans = _hard_trunc(ans,1400) if ans else "ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            lines.append(f"### [{stock}] ë¶€ë¶„ë‹µ\n{ans}\n")
            for s in _fmt_sources(r.get("source_documents", [])):
                source_lines.append(f"[{stock}] {s}")
    
        # ìˆœì„œ ë³´ì¡´ dedup
        seen, dedup = set(), []
        for s in source_lines:
            if s not in seen:
                seen.add(s); dedup.append(s)
    
        # âœ… f-string ì•ˆì— ë°±ìŠ¬ë˜ì‹œê°€ ë“¤ì–´ê°€ë˜ joinì„ ë¯¸ë¦¬ ê³„ì‚°
        parts_joined   = "\n\n".join(lines[:5])
        sources_joined = "\n".join(dedup[:12])
        print(parts_joined)
        print(sources_joined)
    
        prompt = f"""
    ë‹¹ì‹ ì€ ì¦ê¶Œì‚¬ ë¦¬ì„œì¹˜ì„¼í„°ì¥ì…ë‹ˆë‹¤.
    ì•„ë˜ ê° ì¢…ëª©ì˜ ë¶€ë¶„ ë‹µë³€ì„ ì·¨í•©í•˜ì—¬ **ì¢…í•© ë¦¬í¬íŠ¸**ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
    ì¢…í•©ë¦¬í¬íŠ¸ ì‘ì„± ì‹œ ì—­í•  ì„¤ëª…ì€ í•„ìš” ì—†ìœ¼ë©°, ë‹µë³€ ì•ˆì— ìê¸°ì†Œê°œëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.
    ì˜¤ëŠ˜ì´ {self.today}ì„ì„ ê³ ë ¤í•˜ì—¬ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
    
    [ìš”êµ¬ì‚¬í•­]
    1) ì¢…ëª©ë³„ í•µì‹¬ ë‰´ìŠ¤ì™€ ê°€ê²© ì˜í–¥ ê²½ë¡œë¥¼ ë¹„êµ ì •ë¦¬(ê¸/ë¶€ì •, ë‹¨ê¸°/ì¤‘ê¸°)
    2) ê³µí†µ í…Œë§ˆ(ê¸ˆë¦¬, í™˜ìœ¨, ê³µê¸‰ë§, ê·œì œ ë“±) ì‹ë³„ ë° êµì°¨ì˜í–¥ ì„¤ëª…
    3) ì¢…ëª©ë³„ ë¦¬ìŠ¤í¬/ì´‰ë°œìš”ì¸, ëª¨ë‹ˆí„°ë§ ì§€í‘œ ì œì‹œ
    4) ê²°ë¡ : í¬íŠ¸í´ë¦¬ì˜¤ ê´€ì  ì œì–¸(ì˜¤ë²„ì›¨ì´íŠ¸/ë‰´íŠ¸ëŸ´/ì–¸ë”ì›¨ì´íŠ¸ ë“± ì‚¬ìš© ê°€ëŠ¥)
    5) ìˆ˜ì¹˜ëŠ” `ë°±í‹±`ìœ¼ë¡œ, í•µì‹¬ í¬ì¸íŠ¸ëŠ” **êµµê²Œ**, ë¶ˆë¦¿ ì ì ˆ í™œìš©
    6) ëª¨í˜¸í•˜ë©´ 'ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë¶„ëª…íˆ í‘œê¸°
    7) ì¤„ë°”ê¿ˆì€ '<br>' ê°™ì€ HTML íƒœê·¸ ëŒ€ì‹  ì‹¤ì œ ì¤„ë°”ê¿ˆ(ì—”í„°, ê°œí–‰)ìœ¼ë¡œ í‘œì‹œí•  ê²ƒ
    8) ì¤‘ìš” í¬ì¸íŠ¸ ì•ì—ëŠ” ğŸ“Œ, ê¸ì • ìš”ì¸ì—ëŠ” ğŸ“ˆ, ë¦¬ìŠ¤í¬ ìš”ì¸ì—ëŠ” âš ï¸ ê°™ì€ ì´ëª¨ì§€ë¥¼ ë¶™ì´ì„¸ìš”.
    9) ë¦¬í¬íŠ¸ ìƒì„± ì‹œ í‘œ í˜•ì‹ì€ ì‚¬ìš©í•˜ì§€ ë§ ê²ƒ
    
    [ë³´ìœ  ì¢…ëª©ë³„ ìš”ì•½]
    {parts_joined}
    """
        print(f'gen_ai {prompt}')
        try:
            resp = self.gen_model.generate_content(
                prompt, 
                generation_config=self._gen_config(temperature=0.25),
                safety_settings=self._safe_settings())
            text = self._extract_text(resp)
            text = text.replace("<br>", "\n").strip()
            text = text.replace("format:", "").strip()
            return text if text else "ìµœì¢… í†µí•© ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì•ˆì „í•„í„° ë˜ëŠ” í† í° í•œë„ì— ì˜í•´ ì‘ë‹µì´ ë¹„ì—ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            return f"ìµœì¢… í†µí•© ìƒì„± ì˜¤ë¥˜: {e}"

    def answer_5_stocks_and_reduce(self, stocks: List[str], template: Optional[str] = None, max_workers: int = 5) -> Dict[str, Any]:
        template = template or "{stock} ê´€ë ¨í•´ì„œ ì¢…ëª©ì˜ ê°€ê²©ì— ì¤‘ìš”í•œ ë‰´ìŠ¤ëŠ”?"
        per_stock = self.answer_multi_stocks(stocks, template=template, max_workers=max_workers)
        final = self._reduce_across_stocks(template, per_stock)
        return {
            "base_template": template,
            "stocks": stocks,
            "results": per_stock,
            "final_report": final,
        }

    # ---- ì§„ë‹¨ìš©: í•´ë‹¹ ì¢…ëª© ë¬¸ì„œ ìˆ˜ ----
    def count_by_stock(self, stock: str) -> int:
        self._ensure_stock_index()
        try:
            res = self.qc.count(
                collection_name=self.collection,
                count_filter=Filter(must=[FieldCondition(key="stock", match=MatchValue(value=str(stock)))]),
                exact=True,
            )
            return int(getattr(res, "count", 0))
        except Exception:
            return 0


if __name__ == "__main__":
    # âœ… í™˜ê²½ë³€ìˆ˜ í•„ìš”: GOOGLE_CLOUD_PROJECT, QDRANT_URL, QDRANT_API_KEY
    # ë˜ëŠ” Streamlit secrets (gcp_service_account)ë¡œ ì´ˆê¸°í™” ê°€ëŠ¥

    try:
        service = NewsReportService()
    except Exception as e:
        print(f"[Init Error] ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        exit(1)

    # í…ŒìŠ¤íŠ¸ìš© ì¢…ëª© (ì›í•˜ëŠ” í‹°ì»¤/ì‹¬ë³¼ë¡œ êµì²´ ê°€ëŠ¥)
    test_stocks = ["ì‚¼ì„±ì „ì", "í˜„ëŒ€ì°¨", "ì¹´ì¹´ì˜¤", "ë„¤ì´ë²„", "LGì—ë„ˆì§€ì†”ë£¨ì…˜"]

    # ê¸°ë³¸ ì§ˆì˜ í…œí”Œë¦¿
    template = "{stock} ê´€ë ¨í•´ì„œ ì¢…ëª©ì˜ ê°€ê²©ì— ì¤‘ìš”í•œ ë‰´ìŠ¤ëŠ”?"

    print(">>> 5ê°œ ì¢…ëª© ë³‘ë ¬ ì§ˆì˜ & ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸")
    result = service.answer_5_stocks_and_reduce(test_stocks, template=template, max_workers=5)

    # ì¢…ëª©ë³„ ê²°ê³¼ ì¶œë ¥
    for r in result["results"]:
        print("=" * 80)
        print(f"[{r['stock']}] ì§ˆë¬¸: {r['question']}")
        print(f"ë¶€ë¶„ë‹µ:\n{r['answer'][:500]}...\n")  # ì•ë¶€ë¶„ë§Œ í‘œì‹œ

    print("=" * 80)
    print(">>> ìµœì¢… í†µí•© ë¦¬í¬íŠ¸:")
    print(result["final_report"])














